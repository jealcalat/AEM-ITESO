{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Teoría de la estimación\n",
    "## 3.4. Regresión logística\n",
    "\n",
    "<p align=\"right\">\n",
    "Autor: Emmanuel Alcalá\n",
    "<br>\n",
    "<a href=\"https://scholar.google.com.mx/citations?hl=en&user=3URusCgAAAAJ&view_op=list_works&sortby=pubdate\">Google Scholar</a>\n",
    "</p>\n",
    "\n",
    "<p align=\"left\">\n",
    "<br>\n",
    "<a href=\"https://jealcalat.github.io/Analisis_multivariado/\">Regresar a la página del curso</a>\n",
    "</p>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO (Least Absolute Shrinkage and Selection Operator) es una técnica de regularización que no solo reduce el sobreajuste sino que también puede ayudar en la selección de características, ya que puede reducir ciertos coeficientes a cero, eliminando esencialmente la característica del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características seleccionadas con LASSO: [ 7  8 10 11 14 15 18 19 20 21 23 24 26 27 28]\n",
      "Nombre de las características seleccionadas con LASSO: ['mean concave points' 'mean symmetry' 'radius error' 'texture error'\n",
      " 'smoothness error' 'compactness error' 'symmetry error'\n",
      " 'fractal dimension error' 'worst radius' 'worst texture' 'worst area'\n",
      " 'worst smoothness' 'worst concavity' 'worst concave points'\n",
      " 'worst symmetry']\n"
     ]
    }
   ],
   "source": [
    "# Importando las bibliotecas necesarias\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargando el dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividiendo los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Es importante escalar los datos cuando se utiliza LASSO\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenando el modelo de regresión logística con regularización L1 (LASSO)\n",
    "clf = LogisticRegression(penalty='l1', solver='saga', max_iter=10000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Coeficientes del modelo\n",
    "coef = clf.coef_\n",
    "\n",
    "# Características seleccionadas\n",
    "selected_features = np.where(coef != 0)[1]\n",
    "print(f\"Características seleccionadas con LASSO: {selected_features}\")\n",
    "# imprimir nombres de las características seleccionadas\n",
    "print(f\"Nombre de las características seleccionadas con LASSO: {data.feature_names[selected_features]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El término `penalty` en regresión logística (y otros modelos lineales) se refiere a la adición de un término de regularización al objetivo de optimización del modelo. El objetivo de esta regularización es evitar el sobreajuste y, en el caso del penalty `l1`, lograr la selección de características al reducir algunos coeficientes a cero.\n",
    "\n",
    "1. **Función Objetivo sin Regularización**:\n",
    "   Sin regularización, la función objetivo de la regresión logística se centra únicamente en minimizar el error entre las predicciones y los valores verdaderos (la log-verosimilitud negativa).\n",
    "\n",
    "2. **Introducción del Penalty `l1`**:\n",
    "   Cuando introducimos el penalty `l1`, estamos añadiendo un término de regularización basado en la norma l1 de los coeficientes. La norma l1 es simplemente la suma de los valores absolutos de los coeficientes.\n",
    "   \n",
    "  $\\text{Función objetivo} = \\text{Log-verosimilitud negativa} + \\lambda \\sum | \\beta_i | $\n",
    "   \n",
    "   Donde $\\beta_i $ son los coeficientes del modelo y $\\lambda $ es un parámetro de regularización, a menudo llamado `C` en sklearn, que controla la fuerza de la regularización (notar que en `sklearn`, valores más altos de `C` significan menos regularización).\n",
    "\n",
    "3. **Selección de Características con Penalty `l1`**:\n",
    "   La belleza del penalty `l1` radica en su capacidad para hacer exactamente cero algunos coeficientes. A medida que aumenta la fuerza de la regularización (es decir,$\\lambda $ aumenta o `C` disminuye), más coeficientes son reducidos a cero. Coeficientes que son cero efectivamente eliminan esa característica del modelo, logrando así la selección de características. \n",
    "\n",
    "El uso del penalty `l1` es especialmente útil cuando se tiene un conjunto de datos con un gran número de características y se quiere reducir la dimensionalidad seleccionando solo las características más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión con las características seleccionadas en el conjunto de prueba: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Creando el nuevo conjunto de datos con las características seleccionadas\n",
    "X_train_scaled_selected = X_train_scaled[:, selected_features]\n",
    "X_test_scaled_selected = X_test_scaled[:, selected_features]\n",
    "\n",
    "# Reentrenando el modelo con las características seleccionadas\n",
    "clf_selected = LogisticRegression(solver='saga', max_iter=10000)\n",
    "clf_selected.fit(X_train_scaled_selected, y_train)\n",
    "\n",
    "# Evaluando el rendimiento con las características seleccionadas en el conjunto de prueba\n",
    "score_selected = clf_selected.score(X_test_scaled_selected, y_test)\n",
    "print(f\"Precisión con las características seleccionadas en el conjunto de prueba: {score_selected:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué es importante escalar los datos con LASSO?\n",
    "\n",
    "La razón más importante es la sensibilidad a la magnitud de las características: Dado que LASSO impone una penalización basada en la magnitud de los coeficientes, características con diferentes escalas resultarán en coeficientes de diferentes magnitudes, incluso si tienen la misma importancia en la predicción. Una característica con una gran escala (por ejemplo, salario que podría estar en decenas de miles) tendrá un coeficiente más pequeño en comparación con una característica con una escala más pequeña (por ejemplo, edad que podría estar entre 0 y 100). La penalización en LASSO afectaría de manera desproporcionada al coeficiente con una escala más pequeña, simplemente debido a su magnitud, lo que podría llevar a la eliminación incorrecta de características importantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
